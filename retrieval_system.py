# -*- coding: utf-8 -*-
"""Retrieval_system.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16AfmPaJcVYKPEXdHEAJQQ2q1_sxvwhpQ
"""

from gensim import similarities


def search_docs(query, lsi, dictionary, corpus):
    vec_bow = dictionary.doc2bow(query.lower().split())
    vec_lsi = lsi[vec_bow]  # convert the query to LSI space
        
    index = similarities.MatrixSimilarity(lsi[corpus])  # transform corpus to LSI space and index it

    # index.save('/tmp/deerwester.index')
    # index = similarities.MatrixSimilarity.load('/tmp/deerwester.index')

    sims = index[vec_lsi]  # perform a similarity query against the corpus
    print(list(enumerate(sims)))  # print (document_number, document_similarity) 2-tuples

    sims = sorted(enumerate(sims), key=lambda item: -item[1])
    
    return sims


    # """## **Importing necessary libraries**


    # """

    # !python setup.py test
    # !python setup.py install

    # # import time

    # import requests
    # from multiprocessing import cpu_count
    # from multiprocessing.pool import ThreadPool
    # from bs4 import BeautifulSoup
    # import re
    # import json
    # import shutil
    # import os
    # import scidownl
    # from scidownl import scihub_download
    # # import selenium

    # from google.colab import drive
    # drive.mount('/content/drive', force_remount=True)

    # """# **Generating a list of page-urls**
    # ###**Each page contains 25 documents**
    # """

    # def create_urls(num):
    #     url_list = ['https://ntrs.nasa.gov/search']
    #     initial = 25
    #     for i in range(num):
    #         url_list.append('https://ntrs.nasa.gov/search?page=%7B%22size%22:25,%22from%22:{}%7D'.format(initial))
    #         initial += 25
    #     return url_list

    # def get_dois(url_list):
        
    #     urls = []
    #     count = 0
    #     for url in url_list:
    #       reqs = requests.get(url)
    #       soup = BeautifulSoup(reqs.content, 'html.parser')
          
    #       script = soup.find_all('script')[-1]
    #       script = script.contents
    #       cont = script[0]
        
    #       links = re.findall(r'(https?://dx.doi[^\s]+)', cont)
    #       final_links = []
    #       for link in links:
    #         index = link.find('&')
    #         final_links.append(link[:index])
    #         final_links
    #       print(links[0])
    #       print(final_links[0])

    # def download_pdf(doi_link):
    #     '''This downloads a pdf given the link to NASA resource'''
        
    #       paper_type = "doi"
    #       for paper in final_links:
    #           out = "/content/drive/MyDrive/NASA Space Apps Challenge/Dataset/paper_{}.pdf".format(count)
    #           paper = "https://doi.org/" + paper[18:]
    #           scihub_download(paper, paper_type=paper_type, out=out)
    #           count += 1

    #     return



    # url_list = create_urls(9)
    # url_list

    # # url = url_list[0]
    # # reqs = requests.get(url)
    # # soup = BeautifulSoup(reqs.content, 'html.parser')

    # # script = soup.find_all('script')[-1]
    # # script = script.contents
    # # script[0]

    # url_list

    # pdf_extraction(url_list)

    # soup